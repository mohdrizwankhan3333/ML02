{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b302302-7cd5-4aa8-b783-7fdd3e721ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1: Overfitting: happens when a model memorizes noise in the training data, hurting performance on new data.\n",
    "Underfitting is when a model is too simple\n",
    "to learn the patterns in the data, leading to poor performance overall. We can adjust model complexity and training data to avoid both.\n",
    "\n",
    "Q2:To reduce overfitting:  we can simplify the model, use less training data, or add techniques like dropout (randomly turning off neurons) to prevent\n",
    "memorization of noise.\n",
    "\n",
    "Q3:Underfitting: occurs when a model is too basic (like a straight line) to capture complex patterns in the data. This can happen if the model is too\n",
    "simple or the training data doesn't have enough relevant information.\n",
    "\n",
    "Q4: The bias-variance tradeoff: is a balancing act. **Bias** is the error from simplifying the model too much.\n",
    "Variance is the error from the model\n",
    "being too sensitive to training data. A complex model can have high variance, fitting the noise but not generalizing well.\n",
    "\n",
    "Q5:Common methods:  to detect overfitting/underfitting include comparing performance on training and validation data (a separate set of data used for\n",
    "evaluation). A large gap suggests overfitting, while similar low performance on both sets suggests underfitting.\n",
    "\n",
    "Q6:Bias: is like underfitting - the model misses the real pattern. **Variance** is like overfitting - the model captures noise instead of the\n",
    "real pattern. High bias models perform poorly overall, while high variance models can be erratic on different data.\n",
    "\n",
    "Q7:Regularization: is a technique to reduce variance and prevent overfitting. Common techniques include adding a penalty term that discourages\n",
    "complex models or weight decay (reducing the influence of large weights). These techniques make the model focus on the overall pattern instead of memorizing\n",
    "noise.'''\n",
    "\n",
    "print('ml02')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
